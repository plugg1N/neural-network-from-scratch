{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Simple MNIST NN from scratch\n","\n","In this notebook, I implemented a simple two-layer neural network and trained it on the MNIST digit recognizer dataset. It's meant to be an instructional example, through which you can understand the underlying math of neural networks better."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","\n","data = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = np.array(data)\n","m, n = data.shape\n","np.random.shuffle(data) # shuffle before splitting into dev and training sets\n","\n","data_dev = data[0:1000].T\n","Y_dev = data_dev[0]\n","X_dev = data_dev[1:n]\n","X_dev = X_dev / 255.\n","\n","data_train = data[1000:m].T\n","Y_train = data_train[0]\n","X_train = data_train[1:n]\n","X_train = X_train / 255.\n","_,m_train = X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["Y_train"]},{"cell_type":"markdown","metadata":{},"source":["Our NN will have a simple two-layer architecture. Input layer $a^{[0]}$ will have 784 units corresponding to the 784 pixels in each 28x28 input image. A hidden layer $a^{[1]}$ will have 10 units with ReLU activation, and finally our output layer $a^{[2]}$ will have 10 units corresponding to the ten digit classes with softmax activation.\n","\n","**Forward propagation**\n","\n","$$Z^{[1]} = W^{[1]} X + b^{[1]}$$\n","$$A^{[1]} = g_{\\text{ReLU}}(Z^{[1]}))$$\n","$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$$\n","$$A^{[2]} = g_{\\text{softmax}}(Z^{[2]})$$"]},{"cell_type":"markdown","metadata":{},"source":["**Backward propagation**\n","\n","$$dZ^{[2]} = A^{[2]} - Y$$\n","$$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$$\n","$$dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}$$\n","$$dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (z^{[1]})$$\n","$$dW^{[1]} = \\frac{1}{m} dZ^{[1]} A^{[0]T}$$\n","$$dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}$$"]},{"cell_type":"markdown","metadata":{},"source":["**Parameter updates**\n","\n","$$W^{[2]} := W^{[2]} - \\alpha dW^{[2]}$$\n","$$b^{[2]} := b^{[2]} - \\alpha db^{[2]}$$\n","$$W^{[1]} := W^{[1]} - \\alpha dW^{[1]}$$\n","$$b^{[1]} := b^{[1]} - \\alpha db^{[1]}$$"]},{"cell_type":"markdown","metadata":{},"source":["**Vars and shapes**\n","\n","Forward prop\n","\n","- $A^{[0]} = X$: 784 x m\n","- $Z^{[1]} \\sim A^{[1]}$: 10 x m\n","- $W^{[1]}$: 10 x 784 (as $W^{[1]} A^{[0]} \\sim Z^{[1]}$)\n","- $B^{[1]}$: 10 x 1\n","- $Z^{[2]} \\sim A^{[2]}$: 10 x m\n","- $W^{[1]}$: 10 x 10 (as $W^{[2]} A^{[1]} \\sim Z^{[2]}$)\n","- $B^{[2]}$: 10 x 1"]},{"cell_type":"markdown","metadata":{},"source":["Backprop\n","\n","- $dZ^{[2]}$: 10 x m ($~A^{[2]}$)\n","- $dW^{[2]}$: 10 x 10\n","- $dB^{[2]}$: 10 x 1\n","- $dZ^{[1]}$: 10 x m ($~A^{[1]}$)\n","- $dW^{[1]}$: 10 x 10\n","- $dB^{[1]}$: 10 x 1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def init_params():\n","    W1 = np.random.rand(10, 784) - 0.5\n","    b1 = np.random.rand(10, 1) - 0.5\n","    W2 = np.random.rand(10, 10) - 0.5\n","    b2 = np.random.rand(10, 1) - 0.5\n","    return W1, b1, W2, b2\n","\n","def ReLU(Z):\n","    return np.maximum(Z, 0)\n","\n","def softmax(Z):\n","    A = np.exp(Z) / sum(np.exp(Z))\n","    return A\n","    \n","def forward_prop(W1, b1, W2, b2, X):\n","    Z1 = W1.dot(X) + b1\n","    A1 = ReLU(Z1)\n","    Z2 = W2.dot(A1) + b2\n","    A2 = softmax(Z2)\n","    return Z1, A1, Z2, A2\n","\n","def ReLU_deriv(Z):\n","    return Z > 0\n","\n","def one_hot(Y):\n","    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n","    one_hot_Y[np.arange(Y.size), Y] = 1\n","    one_hot_Y = one_hot_Y.T\n","    return one_hot_Y\n","\n","def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n","    one_hot_Y = one_hot(Y)\n","    dZ2 = A2 - one_hot_Y\n","    dW2 = 1 / m * dZ2.dot(A1.T)\n","    db2 = 1 / m * np.sum(dZ2)\n","    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n","    dW1 = 1 / m * dZ1.dot(X.T)\n","    db1 = 1 / m * np.sum(dZ1)\n","    return dW1, db1, dW2, db2\n","\n","def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n","    W1 = W1 - alpha * dW1\n","    b1 = b1 - alpha * db1    \n","    W2 = W2 - alpha * dW2  \n","    b2 = b2 - alpha * db2    \n","    return W1, b1, W2, b2"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_predictions(A2):\n","    return np.argmax(A2, 0)\n","\n","def get_accuracy(predictions, Y):\n","    print(predictions, Y)\n","    return np.sum(predictions == Y) / Y.size\n","\n","def gradient_descent(X, Y, alpha, iterations):\n","    W1, b1, W2, b2 = init_params()\n","    for i in range(iterations):\n","        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n","        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n","        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n","        if i % 10 == 0:\n","            print(\"Iteration: \", i)\n","            predictions = get_predictions(A2)\n","            print(get_accuracy(predictions, Y))\n","    return W1, b1, W2, b2"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500)"]},{"cell_type":"markdown","metadata":{},"source":["~85% accuracy on training set."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def make_predictions(X, W1, b1, W2, b2):\n","    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n","    predictions = get_predictions(A2)\n","    return predictions\n","\n","def test_prediction(index, W1, b1, W2, b2):\n","    current_image = X_train[:, index, None]\n","    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)\n","    label = Y_train[index]\n","    print(\"Prediction: \", prediction)\n","    print(\"Label: \", label)\n","    \n","    current_image = current_image.reshape((28, 28)) * 255\n","    plt.gray()\n","    plt.imshow(current_image, interpolation='nearest')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Let's look at a couple of examples:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_prediction(0, W1, b1, W2, b2)\n","test_prediction(1, W1, b1, W2, b2)\n","test_prediction(2, W1, b1, W2, b2)\n","test_prediction(3, W1, b1, W2, b2)"]},{"cell_type":"markdown","metadata":{},"source":["Finally, let's find the accuracy on the dev set:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dev_predictions = make_predictions(X_dev, W1, b1, W2, b2)\n","get_accuracy(dev_predictions, Y_dev)"]},{"cell_type":"markdown","metadata":{},"source":["Still 84 percent accuracy, so our model generalized from the training data pretty well"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
